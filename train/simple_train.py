# -*- coding: utf-8 -*-
"""
ç®€åŒ–è®­ç»ƒè„šæœ¬ - é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥æ”¶é…ç½®
é¿å…åŠ¨æ€ä¿®æ”¹è„šæœ¬å¸¦æ¥çš„ç¼©è¿›é—®é¢˜

ğŸ¯ éšæœºç§å­æ§åˆ¶è¯´æ˜ (è®ºæ–‡çº§åˆ«çš„å¯é‡å¤æ€§ä¿è¯):
=======================================================

æœ¬è„šæœ¬å·²å®ç°å…¨é¢çš„éšæœºç§å­æ§åˆ¶ï¼Œç¡®ä¿å®éªŒçš„å®Œå…¨å¯é‡å¤æ€§ï¼Œæ»¡è¶³å­¦æœ¯è®ºæ–‡å‘è¡¨è¦æ±‚ï¼š

1. ğŸ² å…¨å±€éšæœºç§å­è®¾ç½®:
   - Python randomæ¨¡å—
   - NumPyéšæœºæ•°ç”Ÿæˆå™¨  
   - PyTorch CPUéšæœºæ•°ç”Ÿæˆå™¨
   - PyTorch CUDAéšæœºæ•°ç”Ÿæˆå™¨
   - CUDNNç¡®å®šæ€§æ¨¡å¼

2. ğŸ“Š æ•°æ®ç›¸å…³éšæœºæ€§æ§åˆ¶:
   - æ•°æ®é›†åˆ†å‰² (train_test_split)
   - æ•°æ®å¢å¼º (RandomHorizontalFlip, RandomRotation, RandomAffine, ColorJitter)
   - æ•°æ®é‡‡æ · (balanced_subsample)
   - DataLoaderæ‰¹æ¬¡é¡ºåº (shuffle=True with generator)
   - å¤šè¿›ç¨‹æ•°æ®åŠ è½½ (worker_init_fn)

3. ğŸ§  æ¨¡å‹ç›¸å…³éšæœºæ€§æ§åˆ¶:
   - æƒé‡åˆå§‹åŒ– (xavier_uniform, kaiming_normalç­‰)
   - Dropoutå±‚éšæœºæ€§
   - æ¨¡å‹æ¶æ„ä¸­çš„éšæœºç»„ä»¶

4. ğŸ”§ è®­ç»ƒç›¸å…³éšæœºæ€§æ§åˆ¶:
   - ä¼˜åŒ–å™¨åˆå§‹åŒ–
   - å­¦ä¹ ç‡è°ƒåº¦å™¨
   - æ¯ä¸ªepochçš„éšæœºæ€§
   - æ¢¯åº¦è®¡ç®—ä¸­çš„éšæœºæ€§

5. ğŸ¯ ç§å­åˆ†é…ç­–ç•¥:
   - ä¸»ç§å­: args.seed
   - æ•°æ®å¢å¼º: args.seed + 1
   - è®­ç»ƒé‡‡æ ·: args.seed + 2  
   - éªŒè¯é‡‡æ ·: args.seed + 3
   - è®­ç»ƒDataLoader: args.seed + 4
   - éªŒè¯DataLoader: args.seed + 5
   - æƒé‡åˆå§‹åŒ–: args.seed + 100
   - æ¯ä¸ªepoch: args.seed + epoch * 1000

ä½¿ç”¨æ–¹æ³•:
python simple_train.py --model_type lightweight_kan --max_epochs 30 --seed 42

è¿™ç¡®ä¿äº†:
âœ… å®Œå…¨çš„å®éªŒå¯é‡å¤æ€§
âœ… å…¬å¹³çš„æ¨¡å‹å¯¹æ¯”
âœ… ç¬¦åˆå­¦æœ¯è®ºæ–‡æ ‡å‡†
âœ… ä¾¿äºè°ƒè¯•å’ŒéªŒè¯
"""

import sys
import os
import argparse

# æ·»åŠ çˆ¶ç›®å½•åˆ°æ¨¡å—è·¯å¾„
sys.path.append(os.path.abspath("E:\GAN_ready\pythonProject"))
import random
from model_convKan_improved import (
    ImprovedKANC_MLP, ResidualKANC_MLP, LightweightKANC_MLP,
    TraditionalCNN, ResidualCNN, LightweightCNN, VGG16, VGG16_Lightweight,
    DeepKANC_MLP, UltraDeepKANC_MLP, CoAtNet_Simple
)
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset, ConcatDataset
from tqdm import tqdm
from htru1 import HTRU1
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import time

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"ğŸ¯ éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")

def worker_init_fn(worker_id):
    """DataLoaderå·¥ä½œè¿›ç¨‹çš„éšæœºç§å­åˆå§‹åŒ–å‡½æ•°"""
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

# ä¿®å¤Windowså¤šè¿›ç¨‹é—®é¢˜
if __name__ == '__main__':
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒæŒ‡å®šçš„æ¨¡å‹')
    parser.add_argument('--model_type', type=str, required=True,
                       choices=['improved_kan', 'residual_kan', 'lightweight_kan',
                               'traditional_cnn', 'residual_cnn', 'lightweight_cnn', 'vgg16', 'vgg16_lightweight',
                               'deep_kan', 'ultra_deep_kan', 'coatnet', 'coatnet_small'],
                       help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--max_epochs', type=int, default=30, help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"é€‰æ‹©çš„æ¨¡å‹: {args.model_type}")
    print(f"è®­ç»ƒè½®æ•°: {args.max_epochs}")
    print(f"éšæœºç§å­: {args.seed}")

    # æ”¹è¿›çš„æ•°æ®å¢å¼ºç­–ç•¥ - ä½¿ç”¨å›ºå®šçš„éšæœºç§å­
    # åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨æ¥æ§åˆ¶æ•°æ®å¢å¼ºçš„éšæœºæ€§
    def create_transform_with_seed(seed_offset=0):
        """åˆ›å»ºå¸¦æœ‰å›ºå®šéšæœºç§å­çš„æ•°æ®å˜æ¢"""
        # ä¸ºæ•°æ®å¢å¼ºè®¾ç½®ç‹¬ç«‹çš„éšæœºç§å­
        transform_seed = args.seed + seed_offset
        torch.manual_seed(transform_seed)
        np.random.seed(transform_seed)
        random.seed(transform_seed)
        
        return transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(15),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

    transform_train = create_transform_with_seed(seed_offset=1)

    # æµ‹è¯•æ—¶ä¸åšæ•°æ®å¢å¼º
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    # é‡æ–°è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿åç»­æ“ä½œçš„ä¸€è‡´æ€§
    set_seed(args.seed)

    # åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ˆä¸ä½¿ç”¨é¢„å®šä¹‰çš„train/teståˆ†å‰²ï¼‰
    print("ğŸ”„ åŠ è½½å¹¶é‡æ–°åˆ†å‰²æ•°æ®é›†...")
    
    # å…ˆåŠ è½½æ‰€æœ‰æ•°æ®ï¼ˆä½¿ç”¨train=Trueè·å–æ›´å¤šæ•°æ®ï¼Œç„¶åè‡ªå·±åˆ†å‰²ï¼‰
    full_dataset = HTRU1('data', train=True, download=True, transform=None)  # å…ˆä¸åº”ç”¨transform
    
    # è·å–æ‰€æœ‰æ ·æœ¬çš„ç´¢å¼•å’Œæ ‡ç­¾
    all_indices = list(range(len(full_dataset)))
    all_labels = [full_dataset.targets[i] for i in all_indices]
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®é›†ç»Ÿè®¡:")
    unique_labels, counts = np.unique(all_labels, return_counts=True)
    for label, count in zip(unique_labels, counts):
        label_name = "è„‰å†²æ˜Ÿ" if label == 1 else "éè„‰å†²æ˜Ÿ"
        print(f"   {label_name} (ç±»åˆ«{label}): {count} ä¸ªæ ·æœ¬")
    
    # åˆ†å±‚åˆ†å‰²ï¼š80%è®­ç»ƒï¼Œ20%éªŒè¯ï¼ˆä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰
    train_indices, val_indices = train_test_split(
        all_indices, 
        test_size=0.2, 
        random_state=args.seed,  # ä½¿ç”¨ä¼ å…¥çš„éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        stratify=all_labels  # åˆ†å±‚æŠ½æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹
    )
    
    print(f"\nğŸ”„ æ•°æ®åˆ†å‰²ç»“æœ:")
    print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_indices)}")
    print(f"   éªŒè¯é›†æ ·æœ¬æ•°: {len(val_indices)}")
    
    # æ£€æŸ¥åˆ†å‰²åçš„ç±»åˆ«åˆ†å¸ƒ
    train_labels = [all_labels[i] for i in train_indices]
    val_labels = [all_labels[i] for i in val_indices]
    
    train_unique, train_counts = np.unique(train_labels, return_counts=True)
    val_unique, val_counts = np.unique(val_labels, return_counts=True)
    
    print(f"\nğŸ“Š è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
    for label, count in zip(train_unique, train_counts):
        label_name = "è„‰å†²æ˜Ÿ" if label == 1 else "éè„‰å†²æ˜Ÿ"
        percentage = count / len(train_labels) * 100
        print(f"   {label_name}: {count} ä¸ªæ ·æœ¬ ({percentage:.1f}%)")
    
    print(f"\nğŸ“Š éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:")
    for label, count in zip(val_unique, val_counts):
        label_name = "è„‰å†²æ˜Ÿ" if label == 1 else "éè„‰å†²æ˜Ÿ"
        percentage = count / len(val_labels) * 100
        print(f"   {label_name}: {count} ä¸ªæ ·æœ¬ ({percentage:.1f}%)")

    # è¿›ä¸€æ­¥é‡‡æ ·ä»¥å¹³è¡¡è®­ç»ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
    def balanced_subsample(indices, labels, pos_samples=1000, neg_samples=1000, seed_offset=0):
        """ä»ç»™å®šç´¢å¼•ä¸­å¹³è¡¡é‡‡æ ·"""
        # ä¸ºé‡‡æ ·è®¾ç½®ç‹¬ç«‹çš„éšæœºç§å­
        sampling_seed = args.seed + seed_offset
        random.seed(sampling_seed)
        np.random.seed(sampling_seed)
        
        # è·å–æ­£è´Ÿæ ·æœ¬ç´¢å¼•
        pos_indices = [idx for idx in indices if all_labels[idx] == 1]  # è„‰å†²æ˜Ÿ
        neg_indices = [idx for idx in indices if all_labels[idx] == 0]  # éè„‰å†²æ˜Ÿ
        
        # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ•°é‡
        pos_samples = min(pos_samples, len(pos_indices))
        neg_samples = min(neg_samples, len(neg_indices))
        
        # éšæœºé‡‡æ ·
        selected_pos = random.sample(pos_indices, pos_samples)
        selected_neg = random.sample(neg_indices, neg_samples)
        
        selected_indices = selected_pos + selected_neg
        random.shuffle(selected_indices)  # æ‰“ä¹±é¡ºåº
        
        print(f"   é‡‡æ ·ç»“æœ: {pos_samples} ä¸ªè„‰å†²æ˜Ÿ + {neg_samples} ä¸ªéè„‰å†²æ˜Ÿ = {len(selected_indices)} ä¸ªæ ·æœ¬")
        return selected_indices

    # å¯¹è®­ç»ƒé›†è¿›è¡Œå¹³è¡¡é‡‡æ ·ï¼ˆæ ¹æ®éœ€è¦è°ƒæ•´æ ·æœ¬æ•°ï¼‰
    train_sampled_indices = balanced_subsample(train_indices, train_labels, 
                                             pos_samples=1000, neg_samples=2000, seed_offset=2)
    
    # å¯¹éªŒè¯é›†ä¹Ÿå¯ä»¥è¿›è¡Œé‡‡æ ·ï¼ˆä¿æŒæ›´å¹³è¡¡çš„è¯„ä¼°ï¼‰
    val_sampled_indices = balanced_subsample(val_indices, val_labels,
                                           pos_samples=200, neg_samples=1800, seed_offset=3)
    
    print(f"\nâœ… æœ€ç»ˆæ•°æ®é›†:")
    print(f"   è®­ç»ƒé›†: {len(train_sampled_indices)} ä¸ªæ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_sampled_indices)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºä¸“é—¨çš„æ•°æ®é›†ç±»æ¥åº”ç”¨ä¸åŒçš„transforms
    class IndexedDataset:
        def __init__(self, base_dataset, indices, transform=None):
            self.base_dataset = base_dataset
            self.indices = indices
            self.transform = transform
        
        def __len__(self):
            return len(self.indices)
        
        def __getitem__(self, idx):
            real_idx = self.indices[idx]
            image, label = self.base_dataset[real_idx]
            
            if self.transform:
                image = self.transform(image)
            else:
                # é»˜è®¤è½¬æ¢
                if not isinstance(image, torch.Tensor):
                    image = transforms.ToTensor()(image)
                    
            return image, label

    # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    train_dataset = IndexedDataset(full_dataset, train_sampled_indices, transform_train)
    val_dataset = IndexedDataset(full_dataset, val_sampled_indices, transform_test)

    print(f"è®­ç»ƒæ•°æ®é›†é•¿åº¦: {len(train_dataset)}")
    print(f"éªŒè¯æ•°æ®é›†é•¿åº¦: {len(val_dataset)}")

    # æ•°æ®åŠ è½½å™¨ - ä¿®å¤Windowså¤šè¿›ç¨‹é—®é¢˜ï¼Œè®¾ç½®num_workers=0ï¼Œå¹¶æ·»åŠ éšæœºç§å­æ§åˆ¶
    # åˆ›å»ºè‡ªå®šä¹‰çš„ç”Ÿæˆå™¨æ¥æ§åˆ¶DataLoaderçš„éšæœºæ€§
    def create_dataloader_generator(seed_offset=0):
        """åˆ›å»ºDataLoaderä½¿ç”¨çš„éšæœºæ•°ç”Ÿæˆå™¨"""
        g = torch.Generator()
        g.manual_seed(args.seed + seed_offset)
        return g

    train_generator = create_dataloader_generator(seed_offset=4)
    val_generator = create_dataloader_generator(seed_offset=5)

    trainloader = DataLoader(
        train_dataset, 
        batch_size=64, 
        shuffle=True, 
        num_workers=0,
        generator=train_generator,
        worker_init_fn=worker_init_fn
    )
    testloader = DataLoader(
        val_dataset, 
        batch_size=64, 
        shuffle=False, 
        num_workers=0,
        generator=val_generator,
        worker_init_fn=worker_init_fn
    )

    # é‡æ–°è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ¨¡å‹åˆå§‹åŒ–çš„ä¸€è‡´æ€§
    set_seed(args.seed)

    # æ¨¡å‹å®ä¾‹åŒ–
    def create_model(model_type):
        if model_type == "improved_kan":
            return ImprovedKANC_MLP(device=device, num_classes=2, dropout_rate=0.5)
        elif model_type == "residual_kan":
            return ResidualKANC_MLP(device=device, num_classes=2, dropout_rate=0.3)
        elif model_type == "lightweight_kan":
            return LightweightKANC_MLP(device=device, num_classes=2)
        elif model_type == "traditional_cnn":
            return TraditionalCNN(num_classes=2, dropout_rate=0.5)
        elif model_type == "residual_cnn":
            return ResidualCNN(num_classes=2, dropout_rate=0.3)
        elif model_type == "lightweight_cnn":
            return LightweightCNN(num_classes=2)
        elif model_type == "vgg16":
            return VGG16(num_classes=2, dropout_rate=0.5)
        elif model_type == "vgg16_lightweight":
            return VGG16_Lightweight(num_classes=2)
        elif model_type == "deep_kan":
            return DeepKANC_MLP(device=device, num_classes=2, dropout_rate=0.5)
        elif model_type == "ultra_deep_kan":
            return UltraDeepKANC_MLP(device=device, num_classes=2, dropout_rate=0.5)
        elif model_type == "coatnet":
            return CoAtNet_Simple(num_classes=2, dropout=0.1)
        elif model_type == "coatnet_small":
            return CoAtNet_Simple(num_classes=2, dropout=0.1)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")

    model = create_model(args.model_type)

    # ç¡®ä¿æ¨¡å‹æƒé‡åˆå§‹åŒ–çš„éšæœºæ€§å—æ§
    # å¯¹äºæœ‰è‡ªå®šä¹‰åˆå§‹åŒ–çš„æ¨¡å‹ï¼Œé‡æ–°åº”ç”¨åˆå§‹åŒ–
    if hasattr(model, '_initialize_weights'):
        # é‡æ–°è®¾ç½®ç§å­åé‡æ–°åˆå§‹åŒ–æƒé‡
        set_seed(args.seed + 100)  # ä½¿ç”¨ç¨å¾®ä¸åŒçš„ç§å­é¿å…ä¸å…¶ä»–ç»„ä»¶å†²çª
        model._initialize_weights()
        print("ğŸ¯ æ¨¡å‹æƒé‡å·²é‡æ–°åˆå§‹åŒ–ï¼ˆå—éšæœºç§å­æ§åˆ¶ï¼‰")

    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

    param_count = count_parameters(model)
    print(f"æ¨¡å‹å‚æ•°é‡: {param_count:,}")

    # ä½¿ç”¨DataParallelï¼ˆå¦‚æœæœ‰å¤šä¸ªGPUï¼‰
    if torch.cuda.device_count() > 1:
        print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        model = nn.DataParallel(model)

    model = model.to(device)

    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
    def calculate_class_weights(dataset):
        labels = []
        # æ ¹æ®æ–°çš„æ•°æ®é›†ç»“æ„è·å–æ ‡ç­¾
        if hasattr(dataset, 'indices'):
            # å¯¹äºIndexedDataset
            for idx in dataset.indices:
                labels.append(full_dataset.targets[idx])
        else:
            # å¯¹äºä¼ ç»Ÿæ•°æ®é›†
            for _, label in dataset:
                labels.append(label)
        
        unique_labels, counts = np.unique(labels, return_counts=True)
        total = len(labels)
        weights = [total / (len(unique_labels) * count) for count in counts]
        return torch.FloatTensor(weights)

    class_weights = calculate_class_weights(train_dataset).to(device)
    print(f"ç±»åˆ«æƒé‡: {class_weights}")
    
    # é’ˆå¯¹æ·±åº¦KANæ¨¡å‹çš„å¬å›ç‡ä¼˜åŒ–é…ç½®
    if 'deep_kan' in args.model_type or 'ultra_deep' in args.model_type:
        # ä¸ºå¬å›ç‡ä¼˜åŒ–è°ƒæ•´ç±»åˆ«æƒé‡ - å¢åŠ æ­£ç±»æƒé‡
        if len(class_weights) == 2:
            # å¢å¼ºæ­£ç±»æƒé‡ä»¥æå‡å¬å›ç‡
            enhanced_weights = class_weights.clone()
            enhanced_weights[1] = enhanced_weights[1] * 1.5  # æ­£ç±»æƒé‡å¢åŠ 50%
            class_weights = enhanced_weights
            print(f"ğŸ“ˆ å¬å›ç‡ä¼˜åŒ– - è°ƒæ•´åç±»åˆ«æƒé‡: {class_weights}")
            
        # ä½¿ç”¨Focal Lossæ¥è¿›ä¸€æ­¥ä¼˜åŒ–å¬å›ç‡
        from torch.nn import CrossEntropyLoss
        
        class FocalLoss(nn.Module):
            """Focal Lossä¸“é—¨ç”¨äºæå‡å¬å›ç‡"""
            def __init__(self, alpha=0.75, gamma=2.0, weight=None):
                super().__init__()
                self.alpha = alpha
                self.gamma = gamma
                self.weight = weight
                self.ce_loss = CrossEntropyLoss(weight=weight, reduction='none')
            
            def forward(self, inputs, targets):
                ce_loss = self.ce_loss(inputs, targets)
                pt = torch.exp(-ce_loss)
                
                # å¯¹æ­£ç±»ç»™äºˆæ›´å¤šå…³æ³¨
                alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
                focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss
                
                return focal_loss.mean()
        
        criterion = FocalLoss(alpha=0.8, gamma=2.0, weight=class_weights)  # æ›´å…³æ³¨æ­£ç±»
        print("ğŸ“ˆ ä½¿ç”¨Focal Lossä¼˜åŒ–å¬å›ç‡")
    else:
        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    # éªŒè¯ä¸€ä¸‹æ•°æ®é›†ç¡®å®åˆ†ç¦»
    train_set = set(train_sampled_indices)
    val_set = set(val_sampled_indices)
    overlap = train_set & val_set
    print(f"\nâœ… æ•°æ®åˆ†ç¦»éªŒè¯:")
    print(f"   è®­ç»ƒé›†å’ŒéªŒè¯é›†é‡å æ ·æœ¬æ•°: {len(overlap)}")
    if len(overlap) == 0:
        print("   âœ… æ•°æ®é›†å®Œå…¨åˆ†ç¦»ï¼Œæ— æ•°æ®æ³„éœ²é£é™©")
    else:
        print("   âŒ è­¦å‘Šï¼šå­˜åœ¨æ•°æ®æ³„éœ²é£é™©ï¼")

    # é‡æ–°è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ä¼˜åŒ–å™¨åˆå§‹åŒ–çš„ä¸€è‡´æ€§
    set_seed(args.seed)

    # ä¼˜åŒ–å™¨ï¼ˆé’ˆå¯¹æ·±åº¦KANæ¨¡å‹è°ƒæ•´å­¦ä¹ ç‡ï¼‰
    if 'deep_kan' in args.model_type or 'ultra_deep' in args.model_type:
        # æ·±åº¦æ¨¡å‹ä½¿ç”¨ç¨ä½çš„å­¦ä¹ ç‡
        initial_lr = 5e-4
        print(f"ğŸ“ˆ æ·±åº¦KANæ¨¡å‹ä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡: {initial_lr}")
    else:
        initial_lr = 1e-3
    
    # åˆ›å»ºä¼˜åŒ–å™¨æ—¶ç¡®ä¿éšæœºæ€§æ§åˆ¶
    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=1e-4, betas=(0.9, 0.999))

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.max_epochs, eta_min=1e-6)

    # å†æ¬¡è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ä¸€è‡´æ€§
    set_seed(args.seed)
    
    print(f"\nğŸ¯ æ‰€æœ‰éšæœºæ€§ç»„ä»¶å·²è®¾ç½®å®Œæˆï¼Œä½¿ç”¨ç§å­: {args.seed}")
    print("   âœ… æ•°æ®å¢å¼ºéšæœºæ€§å·²æ§åˆ¶")
    print("   âœ… æ•°æ®é‡‡æ ·éšæœºæ€§å·²æ§åˆ¶") 
    print("   âœ… DataLoaderéšæœºæ€§å·²æ§åˆ¶")
    print("   âœ… æ¨¡å‹åˆå§‹åŒ–éšæœºæ€§å·²æ§åˆ¶")
    print("   âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–éšæœºæ€§å·²æ§åˆ¶")
    print("   âœ… è®­ç»ƒè¿‡ç¨‹éšæœºæ€§å·²æ§åˆ¶")

    # æ—©åœæœºåˆ¶
    class EarlyStopping:
        def __init__(self, patience=7, min_delta=0.001):
            self.patience = patience
            self.min_delta = min_delta
            self.counter = 0
            self.best_loss = float('inf')
        
        def __call__(self, val_loss):
            if val_loss < self.best_loss - self.min_delta:
                self.best_loss = val_loss
                self.counter = 0
                return False
            else:
                self.counter += 1
                return self.counter >= self.patience

    early_stopping = EarlyStopping(patience=10)

    # è®­ç»ƒå‡½æ•°
    def train_epoch(model, trainloader, criterion, optimizer, device):
        # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼ï¼ŒDropoutç­‰å±‚ä¼šæ­£å¸¸å·¥ä½œ
        model.train()
        # ç¡®ä¿Dropoutçš„éšæœºæ€§æ˜¯ç¡®å®šçš„
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        running_loss = 0.0
        correct = 0
        total = 0
        
        with tqdm(trainloader, desc="Training") as pbar:
            for images, labels in pbar:
                images, labels = images.to(device), labels.to(device)
                
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'LR': f'{optimizer.param_groups[0]["lr"]:.6f}'
                })
        
        return running_loss / len(trainloader), 100.0 * correct / total

    # éªŒè¯å‡½æ•°ï¼ˆåœ¨ç‹¬ç«‹çš„éªŒè¯é›†ä¸Šè¯„ä¼°ï¼‰
    def validate_epoch(model, valloader, criterion, device):
        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼ï¼ŒDropoutç­‰å±‚ä¼šè¢«ç¦ç”¨
        model.eval()
        
        running_loss = 0.0
        correct = 0
        total = 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            with tqdm(valloader, desc="Validation") as pbar:
                for images, labels in pbar:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    
                    running_loss += loss.item()
                    _, predicted = torch.max(outputs, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
                    
                    all_predictions.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
                    
                    pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{100.*correct/total:.2f}%'
                    })
        
        # è®¡ç®—å‡é˜³ç‡å’Œå…¶ä»–åˆ†ç±»æŒ‡æ ‡
        cm = confusion_matrix(all_labels, all_predictions)
        
        # å‡è®¾æ ‡ç­¾: 0=è´Ÿç±»(éè„‰å†²æ˜Ÿ), 1=æ­£ç±»(è„‰å†²æ˜Ÿ)
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0
            false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0.0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        else:
            false_positive_rate = 0.0
            false_negative_rate = 0.0
            precision = 0.0
            recall = 0.0
            f1_score = 0.0
        
        accuracy = 100.0 * correct / total
        
        return (running_loss / len(valloader), accuracy, all_predictions, all_labels, 
                false_positive_rate, false_negative_rate, precision, recall, f1_score)

    # ä¸»è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    best_fpr = float('inf')  # æœ€ä½³å‡é˜³ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    fprs, fnrs, precisions, recalls, f1_scores = [], [], [], [], []  # æ–°å¢æŒ‡æ ‡è®°å½•
    
    print(f"\nå¼€å§‹è®­ç»ƒ {args.model_type} æ¨¡å‹...")
    print(f"å‚æ•°é‡: {param_count:,}")
    print("-" * 60)
    
    start_time = time.time()
    
    for epoch in range(args.max_epochs):
        # ä¸ºæ¯ä¸ªepochè®¾ç½®ç¡®å®šæ€§çš„éšæœºç§å­
        epoch_seed = args.seed + epoch * 1000
        set_seed(epoch_seed)
        
        print(f"\nEpoch {epoch+1}/{args.max_epochs} (ç§å­: {epoch_seed})")
        print("-" * 50)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)
        
        # éªŒè¯ï¼ˆæ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯ç‹¬ç«‹çš„éªŒè¯é›†ï¼Œä¸è®­ç»ƒé›†å®Œå…¨åˆ†ç¦»ï¼‰
        val_loss, val_acc, predictions, true_labels, false_positive_rate, false_negative_rate, precision, recall, f1_score = validate_epoch(model, testloader, criterion, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•æŒ‡æ ‡
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        fprs.append(false_positive_rate)
        fnrs.append(false_negative_rate)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1_score)
        
        print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        print(f"å‡é˜³ç‡(FPR): {false_positive_rate:.4f}, å‡é˜´ç‡(FNR): {false_negative_rate:.4f}")
        print(f"ç²¾ç¡®ç‡: {precision:.4f}, å¬å›ç‡: {recall:.4f}, F1åˆ†æ•°: {f1_score:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼Œä½†ä¹Ÿè€ƒè™‘å‡é˜³ç‡ï¼‰
        if val_acc > best_val_acc or (val_acc == best_val_acc and false_positive_rate < best_fpr):
            best_val_acc = val_acc
            best_fpr = false_positive_rate
            if isinstance(model, torch.nn.DataParallel):
                torch.save(model.module.state_dict(), f'./best_model_{args.model_type}.pth')
            else:
                torch.save(model.state_dict(), f'./best_model_{args.model_type}.pth')
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%, å‡é˜³ç‡: {best_fpr:.4f}")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss):
            print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
            break
        
        # æ¯10è½®æ‰“å°è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        if (epoch + 1) % 10 == 0:
            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(true_labels, predictions, 
                                      target_names=['Non-Pulsar', 'Pulsar']))
    
    end_time = time.time()
    training_time = end_time - start_time
    
    # æœ€ç»ˆç»“æœæ€»ç»“
    print("\n" + "="*80)
    print(f"è®­ç»ƒå®Œæˆï¼")
    print(f"æ¨¡å‹ç±»å‹: {args.model_type}")
    print(f"å‚æ•°é‡: {param_count:,}")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"æœ€ä½³å‡é˜³ç‡: {best_fpr:.4f}")
    print("="*80)
    
    # ä¿å­˜è®­ç»ƒå†å²ï¼ˆåŒ…å«æ‰€æœ‰æ–°æŒ‡æ ‡ï¼‰
    results = {
        'model_type': args.model_type,
        'param_count': param_count,
        'training_time': training_time,
        'best_val_acc': best_val_acc,
        'best_fpr': best_fpr,
        'final_fpr': fprs[-1] if fprs else 0.0,
        'final_fnr': fnrs[-1] if fnrs else 0.0,
        'final_precision': precisions[-1] if precisions else 0.0,
        'final_recall': recalls[-1] if recalls else 0.0,
        'final_f1_score': f1_scores[-1] if f1_scores else 0.0,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accs': train_accs,
        'val_accs': val_accs,
        'fprs': fprs,
        'fnrs': fnrs,
        'precisions': precisions,
        'recalls': recalls,
        'f1_scores': f1_scores
    }
    
    torch.save(results, f'./training_results_{args.model_type}.pkl')

if __name__ == '__main__':
    main() 